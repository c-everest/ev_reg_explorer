---
title: "ce_scraping_msrp_data"
description: Scraping historical MSRP data from Autotrader with rvest  - troubleshooting and testing
output: html_document
date: 12-28-20
---
#Objective:
Autotrader has the original MSRP range for virtually all domestic vehicle make/model/year combinations as a page element. 
For example: 
    <https://www.autotrader.com/audi/e-tron/2020> 
    <https://www.autotrader.com/tesla/model-s/2020>
    <https://www.autotrader.com/mini/cooper/2019>
    <https://www.autotrader.com/kia/soul/2019>

![example of original msrp range on autotrader](../autotrader_msrp_example.png)
 Trim level MSRPs are stored in tables.
 ![table with trim-level data](../autotrader_example_kia_soul_trim_table.png)
Initial testing of Selenium triggered the Autotrader site to block requests, but reproducing Autotrader's URL structure for model info pages makes static page scraping straightforward enough.

Load libraries
```{r}
library(tidyverse)
library(dplyr)
library(plotly)
library(ggplot2)
library(httr)
library(stringr)
library(rvest)
library(xml2)  #XML package no longer maintained. xml2 is a newer wrapper
#library(R.utils)
```
#Scrape trim-level MSRP data
#Use XPath's more powerful query options
Only want MSRP data for model trims with electric motors
```{r}
testpage <- read_html("https://www.autotrader.com/kia/soul/2019")

#Ford Focus page has two trim tables, for each hatchback & sedan. Below code works for this as well.
#testpage <- read_html("https://www.autotrader.com/ford/focus/2018")

model_evtrim_msrps<-testpage%>%
  #Select table containing vehicle trim data
  html_nodes(xpath='//div[@data-cmp="trims"]//table//tbody')%>%
  #Select rows td child element is "Electric" and extract td element in MSRP column
  html_nodes(xpath = '//tr[td[text()="!Electric"]]/td[position()=2]')%>%
  html_text()

length(model_evtrim_msrps)
model_evtrim_msrps
```

#Use GET() to access the site
httr GET() vs read_html(url)
"The difference is that with repeated calls to httr::GET, the handle persists between calls. With xml2::read_html(), a new connection is made each time...read_html(GET(url)) is like refreshing your browser, but read_html(url) is like closing your browser and opening a new one. The server gives a unique session ID on the page it delivers. New session, new ID. You can prove this by calling httr::reset_handle(url)"

<https://stackoverflow.com/questions/58219503/difference-between-read-htmlurl-and-read-htmlcontentgeturl-text>
```{r}
request <-GET("https://www.autotrader.com/kia/soul/2019")
request$status_code
#content(request)%>%html_node(".text-left.text-size-400.text-bold")%>%html_text()

model_evtrim_msrps<-content(request)%>%
  #Select table containing vehicle trim data
  html_nodes(xpath='//div[@data-cmp="trims"]//table//tbody')%>%
  #Select rows td child element is "Electric" and extract td element in MSRP column
  html_nodes(xpath = '//tr[td[text()="Electric"]]/td[position()=2]')%>%
  html_text()
model_evtrim_msrps
```

#Extract the range of MSRP values
A single table cell entry scraped from Autotrader includes MSRP/Price, with "price" being current available advertised prices. Only MSRP is relevant. 
```{r}
#Confirm data type of rvest output - character vector
typeof(model_evtrim_msrps)
str(model_evtrim_msrps)

#The printed representation of a string is not the same as the string itself, as it shows the escapes. Use writeLines() to see the raw string values. 
writeLines(model_evtrim_msrps)

x<-model_evtrim_msrps%>%
  #Remove dollar sign from MSRP entry
  str_replace("\\$","")%>%
  #Remove comma from MSRP entry
  str_replace(",","")%>%
  #Extract digits of MSRP
  str_extract("\\d{4,6}")%>%
  as.numeric()
#%>%sort()

#Check to make sure converted to numeric properly
class(x)

#Grab the min and max values
min(x)
max(x)

```



#Write webscraping function
```{r}
#Create column in dataframe and fill with placeholder values
sample_cars$msrp_min <- 'x'
sample_cars$msrp_max <- 'y'

#Scrape MSRP info with for loop
for(i in 1:length(sample_cars$autotrader_url)) {
  #Call the vehicle's specific URL
  request <-GET(sample_cars$autotrader_url[i])
  
  #check the status code
  request$status_code
  
  #if request is successful, proceed to scraping the page
  if (request$status_code==200)
    {
      #extract msrp range for all trim levels with electric motors
      model_evtrim_msrp<-content(request)%>%
        #select table containing vehicle trim data
        html_nodes(xpath='//div[@data-cmp="trims"]//table//tbody')%>%
        #select rows where td child element is "Electric" and extract td element in MSRP column
        html_nodes(xpath = '//tr[td[text()="Electric"]]/td[position()=2]')%>%
        #format as text
        html_text()%>%
        #remove dollar sign from MSRP entries
        str_replace("\\$","")%>%
        #remove comma from MSRP entries
        str_replace(",","")%>%
        #extract digits of MSRPs
        str_extract("\\d{4,6}")%>%
        #change from string to numeric
        as.numeric()
      
      #replace placeholder column values with min and max msrp
      sample_cars$msrp_min[i] <- min(model_evtrim_msrp)
      sample_cars$msrp_max[i] <- max(model_evtrim_msrp)
    }
  
  #if request returns anything other than a 200 'OK' status code, print the status code to the column values
  else 
    {
      #replace placeholder column values
      sample_cars$msrp_min[i] <- request$status_code
      sample_cars$msrp_max[i] <- request$status_code
    }
  
  #pause between requests & vary duration
  #generate sample within given range
  sleep_seconds <- sample(1:5,4)
  #
  Sys.sleep(sleep_seconds[(i-1) %% length(sleep_seconds)+1])
  
}

view(sample_cars)
```


#Parse URLs in Autotrader XML sitemap to create a 'library' of vehicles

Fetch detailed urls for make/model/year pages from sitemap index
```{r}
#get list of urls for relevant portion of the sitemap
#sitemap_index <- read_xml("https://www.autotrader.com/sitemap_mmty.xml.gz")%>%
sitemap_index <- read_xml('../data/sitemap_mmty.xml')%>%
  xml_ns_strip()%>% #strip default namespaces to make find_all work
  xml_find_all("//loc")%>% #target node containing url
  xml_text()

#check length of list - should be 6
length(sitemap_index)

#combine results from each selected portion of the sitemap

#create empty list
#mmty_urls <- list() #QUESTION - IS A LIST THE BEST WAY TO STORE RESULTS OF THE LOOP?
mmty_urls <- c()

for(i in 1:length(sitemap_index)) {
  #collect all urls from make/model/trim/year sitemap pages 0-5
  request <-GET(sitemap_index[i])
  
  print(request$status_code)
  
  #extract urls from each xml nodelist
  mmty_sitemap_chunk <- read_xml(request$content)%>%
    xml_ns_strip()%>% #strip default namespaces to make find_all work
    xml_find_all("//loc")%>%
    xml_text()

  #append each set of urls to list
  mmty_urls <- append(mmty_urls, mmty_sitemap_chunk)
  
  #check length of url list
  print(length(mmty_urls))
}

print(length(mmty_urls))
typeof(mmty_urls)

#str_remove_all("https\\:\\/\\/www\\.autotrader.com\\/")%>%
#str_extract("(?<=sitemap_).*(?=\\.xml\\.gz)")
#str_remove_all("https\\:\\/\\/www\\.autotrader.com\\/")%>%
#str_remove_all("\\?styleid=\\d+") 

```

Transform into dataframe and parse url strings
```{r}
#vector to dataframe
veh_library <- mmty_urls%>%as.data.frame()

glimpse(veh_library) #~52k rows
view(veh_library)

#rename column
colnames(veh_library) <-veh_library%>%
  colnames()%>%
  str_replace_all(".","sitemap_url")

#parse urls, extracting fields for: make, model, year

#extract vehicle model year as new column
veh_library$model_year <- veh_library$sitemap_url%>%
  str_remove_all("\\?styleid=\\d+")%>% #remove styleid
  str_extract("\\d{4}$") #vehicle year

#extract vehicle make as new column
veh_library$make <-veh_library$sitemap_url%>%
  str_remove_all("\\?styleid=\\d+")%>%
  str_remove_all("https\\:\\/\\/www\\.autotrader.com\\/")%>%
  str_extract("^[a-z]+-?[a-z]*")

#extract vehicle model as new column
veh_library$model <- veh_library$sitemap_url%>%
  str_remove_all("\\?styleid=\\d+")%>%
  str_remove_all("https\\:\\/\\/www\\.autotrader.com\\/[a-z]+-?[a-z]*\\/")%>%
  str_extract("^[a-z 0-9 \\- \\~]*(?=\\/)")

#there are still some na values. identify anomalies and update code
navalues <- c("https://www.autotrader.com/dodge/d~w-truck/150/",
"https://www.autotrader.com/dodge/ram-3500-truck/st/",
"https://www.autotrader.com/ford/explorer-sport-trac/xlt/",
"https://www.autotrader.com/ford/e-350-and-econoline-350/xl/",
"https://www.autotrader.com/land-rover/range-rover-sport/hse/",
"https://www.autotrader.com/mazda/mx-5-miata/ls/",
"https://www.autotrader.com/mercedes-benz/c-63-amg/s/",
"https://www.autotrader.com/ram/c~v/tradesman/")

navalues%>%
  str_remove_all("https\\:\\/\\/www\\.autotrader.com\\/[a-z]+-?[a-z]*\\/")%>%
  str_extract("^[a-z 0-9 \\- \\~]*(?=\\/)")

glimpse(veh_library)

#check how many unique make/model/year vehicles there are - 6838
veh_library <- veh_library %>% 
  #filter(make%in%exclude_make==FALSE)%>%
  select(make, model, model_year) %>%
  distinct(.keep_all = False) %>%
  group_by(make)%>%
  arrange(make, model, model_year, .by_group=TRUE)

#spot check make/model/year for potential typos and N/A values

veh_library%>%
  filter(make=="tesla")%>%
  filter(model_year=="2020")

glimpse(veh_library)
```
Write vehicle library to CSV
```{r}
write.csv(veh_library, '../data/veh_library.csv', row.names = FALSE)
```
